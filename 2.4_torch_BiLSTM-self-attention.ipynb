{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108919eb-e2e5-4114-b7bd-49d2496d8f18",
   "metadata": {},
   "source": [
    "### Experiment : Classify Spampinato eeg data\n",
    "**Goal:** Try to verify that spampinato eeg data can be classify\n",
    "</br>\n",
    "**Model:** Bidirectional LSTM with attention\n",
    "</br>\n",
    "**Data:** spampinato eeg\n",
    "</br>\n",
    "**Result:** <font color='green'>Acc 48%</font>\n",
    "</br>\n",
    "**Conclusion:** The accuracy is better than EEGNet and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8bffee0-0544-4eaf-9c22-7731af70315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys, os\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "\n",
    "from libs.EEGModels.EEGNet import EEGNet\n",
    "from libs.Classification_Training import Classification_Training\n",
    "from libs.utilities import save_result_csv\n",
    "\n",
    "from libs.utilities import get_freer_gpu\n",
    "device = torch.device(get_freer_gpu()) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "start_time = time.time()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f2d59d6-a4b8-450a-926f-ae65e93c3498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================\n",
      "---------------Running agument------------------\n",
      "Debug mode is : True\n",
      "Device =  cuda:1\n",
      "participant_id : all\n",
      "model_name : LSTM_attention\n",
      "result_file : result_LSTM_attention\n",
      "n_epochs : 500\n"
     ]
    }
   ],
   "source": [
    "print(\"======================================================\")\n",
    "\n",
    "try:\n",
    "    print(sys.argv[3])\n",
    "    is_debug    = False\n",
    "except IndexError:\n",
    "    is_debug    = True\n",
    "    \n",
    "print(\"---------------Running agument------------------\")\n",
    "print(f'Debug mode is : {is_debug}')\n",
    "\n",
    "\n",
    "if is_debug:\n",
    "    participant_id  = \"all\"\n",
    "    model_name      = \"LSTM_attention\"                  #         EEGNet , CNN1D\n",
    "    model_round     = 1\n",
    "    result_file     = f\"result_{model_name}\"\n",
    "    n_epochs        = 500\n",
    "else:\n",
    "    participant_id      = sys.argv[1]\n",
    "    if participant_id != \"all\":\n",
    "        participant_id = int(participant_id)\n",
    "    model_name          = sys.argv[2]\n",
    "    model_round         = int(sys.argv[3])\n",
    "    result_file         = sys.argv[4]\n",
    "    n_epochs            = 300\n",
    "    \n",
    "\n",
    "print(\"Device = \", device)\n",
    "print(\"participant_id :\" , participant_id)\n",
    "print(\"model_name :\" , model_name)\n",
    "print(\"result_file :\" , result_file)\n",
    "print(\"n_epochs :\", n_epochs)\n",
    "\n",
    "model_save_path    = \"save/weight/Spam_EEG_classify\"\n",
    "workers   =  8 \n",
    "batch_size  = 256\n",
    "\n",
    "running_model= f\"Spam_EEG_classify_{model_name}-p{participant_id}\"\n",
    "running_time = datetime.today().strftime('%Y%m%d-t%H%M%S')\n",
    "env = [running_model,running_time ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a32634-13aa-49d1-abb0-0a524ac05f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim     = 128\n",
    "hidden_dim    = 32\n",
    "num_layers    = 1\n",
    "num_classes   = 40\n",
    "bidirectional = True\n",
    "dropout       = 0\n",
    "learning_rate = 1e-3\n",
    "\n",
    "scheduler = \"StepLR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb4efc3-fea7-4b1f-a841-42a9c41f9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "class Spam_EEG_ds(Dataset):\n",
    "    file_location = 'dataset/spampinoto_dataset/Spam_EEG_14_70.pickle'\n",
    "\n",
    "    # __FILE_VAL_LOC__ = os.path.join(__dirname__, 'content/very_nice_dataset/')\n",
    "\n",
    "    def __init__(self, device, participant_id=1):\n",
    "        super(Spam_EEG_ds, self).__init__()\n",
    "\n",
    "        self.participant_id = participant_id\n",
    "        self.whole_data = pickle.load(open(self.file_location, \"rb\"))\n",
    "        self.curr_participant_data = self.whole_data[self.participant_id]\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        eeg   = self.curr_participant_data[0][idx].to(self.device)\n",
    "        label = self.curr_participant_data[1][idx].to(self.device)\n",
    "        return eeg, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.curr_participant_data[1])\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"spampinato_data\"\n",
    "\n",
    "    def change_participant_id(self, participant_id=1):\n",
    "        self.participant_id        = participant_id\n",
    "        self.curr_participant_data = self.whole_data[self.participant_id]\n",
    "    def get_eeg_shape(self):\n",
    "        return self.curr_participant_data[0].shape\n",
    "    \n",
    "    def set_eeg_shape(self, cnn_type) :\n",
    "        if cnn_type == \"CNN1D\":\n",
    "            print(cnn_type)\n",
    "            print(type( self.curr_participant_data[0]))\n",
    "            #self.curr_participant_data[0] = self.curr_participant_data[0].view(-1,  128, 1,  491 )\n",
    "            #self.curr_participant_data[0] = self.curr_participant_data[0].permute(0 ,2 ,1 ,3)\n",
    "            \n",
    "            cur_eeg   = self.curr_participant_data[0].permute(0 ,2 ,1 ,3)\n",
    "            cur_label = self.curr_participant_data[1]\n",
    "            \n",
    "            self.curr_participant_data = (cur_eeg, cur_label)\n",
    "            \n",
    "        elif cnn_type == \"CNN2D\" or cnn_type==\"EEGNet\":\n",
    "            print(cnn_type)\n",
    "            cur_eeg   = self.curr_participant_data[0].permute(0 ,2 ,1 ,3)\n",
    "            cur_label = self.curr_participant_data[1]\n",
    "            self.curr_participant_data = (cur_eeg, cur_label)\n",
    "        elif \"LSTM\" in cnn_type :\n",
    "            cur_eeg   = torch.squeeze(self.curr_participant_data[0]).permute(0, 2, 1)\n",
    "            cur_label = self.curr_participant_data[1]\n",
    "            self.curr_participant_data = (cur_eeg, cur_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a05ff6-ffe5-4e65-af02-d72bf9b767a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_test_split_ds( dataset_in , test_size=0.1, batch_size=10 ):\n",
    "    seed       = 42\n",
    "\n",
    "    _labels = dataset_in.whole_data[dataset_in.participant_id][1]\n",
    "    # generate indices: instead of the actual data we pass in integers instead\n",
    "    train_indices, test_indices, _, _ = train_test_split(\n",
    "                                        range(len(dataset_in))   ,\n",
    "                                        _labels                   ,\n",
    "                                        stratify     = _labels    ,\n",
    "                                        test_size    = test_size ,\n",
    "                                        random_state = seed\n",
    "                                    )\n",
    "    \n",
    "    # generate subset based on indices\n",
    "    train_split = Subset(dataset_in, train_indices)\n",
    "    test_split  = Subset(dataset_in, test_indices)\n",
    "\n",
    "    # create batches\n",
    "    train_iterator = DataLoader( train_split, batch_size=batch_size, shuffle=True  )\n",
    "    val_iterator   = DataLoader( test_split,  batch_size=batch_size, shuffle=False )\n",
    "    \n",
    "    return train_iterator, val_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1bd0d50-9e38-40f1-adcd-b6bd2adae49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTM_attention(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, seq_len, channels)\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes, bidirectional, dropout):\n",
    "        super(LSTM_attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm       = nn.LSTM(input_dim, \n",
    "                                  hidden_dim, \n",
    "                                  num_layers, \n",
    "                                  bidirectional=bidirectional, \n",
    "                                  dropout=dropout, \n",
    "                                  batch_first=True\n",
    "                                 )\n",
    "        self.fc      = nn.Linear(hidden_dim * num_layers *2 , num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.lin_Q = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.lin_K = nn.Linear()\n",
    "        self.lin_V = nn.Linear()\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        # lstm_output.shape(out)  =  ([256, 491, 64])\n",
    "        # final_state.shape(hn)   =  ([256, 64])\n",
    "        \n",
    "        hidden = final_state.unsqueeze(2)                         # hidden shape : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        #print(\"hidden.shape attention_net: \", hidden.shape)      # ([256, 64, 1])\n",
    "       \n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2)  # attn_weights : [batch_size, seq_len, 1]\n",
    "        #print(\"attn_weights.shape: \", attn_weights.shape )       # ([256, 491])\n",
    "        \n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        # [batch_size, n_hidden * num_directions(=2), seq_len] * [batch_size, seq_len, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        \n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        return \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"X input.shape\", x.shape)\n",
    "        \n",
    "        # Set initial hidden and cell states\n",
    "        #*2 because it's bidirectional \n",
    "        h0 = torch.zeros(self.num_layers * 2 , x.size(0), self.hidden_dim).to(device).float()\n",
    "        c0 = torch.zeros(self.num_layers * 2 , x.size(0), self.hidden_dim).to(device).float()\n",
    "       \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0)) # out.shape : tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        #print(\"out.shape\" , out.shape)  #  ([256, 491, 64])     \n",
    "        #print(hn[-2,:,:].shape)\n",
    "        #print(hn[-1,:,:].shape)\n",
    "\n",
    "        #output, output_lengths = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        #hn = [batch size, hidden dim * num directions]\n",
    "        #print(\"hn.shape\" , hn.shape)  # got ([256, 64])\n",
    "        \n",
    "        attn_output, attention = self.attention_net(out, hn)\n",
    "        out = self.fc( attn_output  )\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20d4036-3bbc-429c-a025-eee9be9e8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_save_result( ) :\n",
    "    \n",
    "    # ==============================================\n",
    "    # modelling\n",
    "    print(\"...... Modelling\")\n",
    "    n_class         = 40\n",
    "    print(\"n_class :\" , n_class)\n",
    "    \n",
    "    model_to_train  = None\n",
    "    eeg_shape = dataset.get_eeg_shape()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if model_name == \"Conv1D_LSTM\" :\n",
    "        print(f\"select {model_name}\")\n",
    "        \n",
    "        if  eeg_shape[2] != 1 or eeg_shape[1] != 128 :\n",
    "            dataset.set_eeg_shape('Conv1D_LSTM')\n",
    "#             print(dataset.get_eeg_shape())\n",
    "            \n",
    "        \n",
    "#         dataset.set_eeg_shape(model_name)\n",
    "\n",
    "        model_to_train = Conv1D_LSTM(input_dim, hidden_dim, num_layers, num_classes, bidirectional, dropout)\n",
    "        model_to_train = model_to_train.float()\n",
    "\n",
    "    elif  \"LSTM\" in model_name :\n",
    "        dataset.set_eeg_shape('LSTM')\n",
    "        \n",
    "        print(f\"select {model_name}\")\n",
    "        print(\"dataset.get_eeg_shape() \" , dataset.get_eeg_shape() )\n",
    "\n",
    "#         dataset.set_eeg_shape(model_name)\n",
    "\n",
    "        model_to_train = LSTM_attention(input_dim, hidden_dim, num_layers, num_classes, bidirectional, dropout)\n",
    "        model_to_train = model_to_train.float() #define precision as float to reduce running time\n",
    "\n",
    "        \n",
    "    training_obj                 = Classification_Training( model_obj = model_to_train ,  device= device ,learning_rate=learning_rate, criterion=nn.CrossEntropyLoss()  , env=env, visdom_update=False )\n",
    "    training_obj.is_debug        = is_debug\n",
    "    training_obj.is_plot_graph   = is_debug\n",
    "    training_obj.do_training(   train_loader, val_loader , scheduler=scheduler,  n_epochs=n_epochs )\n",
    "\n",
    "#     #=================================================\n",
    "#     # save model weight \n",
    "#     print(\"..... Save model weight  \" )\n",
    "\n",
    "#     training_obj.save_state_dict(  f'{model_save_path}' )\n",
    "    \n",
    "    # ==============================================\n",
    "    print(\"..... Save result to csv file\")\n",
    "\n",
    "    result_row = training_obj.get_result()\n",
    "    # [ type(self.best_model).__name__ ,  self.best_train_acc, self.best_val_acc, self.best_test_acc , self.best_epoch ]\n",
    "    data_name     = \"Spampinato\"\n",
    "    par_name      =  participant_id\n",
    "    result_row.insert(0,data_name)\n",
    "    result_row.insert(1,par_name)\n",
    "    result_row.insert(3,model_round)\n",
    "\n",
    "#     headers =  ['data_name', 'participant', 'model_round', 'model_name', 'train_acc', 'val_acc', 'test_acc', 'best_epoch']\n",
    "    headers =  ['data_name', 'participant',  'model_name', 'model_round', 'train_acc', 'val_acc', 'test_acc', 'best_epoch']\n",
    "    save_result_csv(headers, result_row, f\"{result_file}.csv\")\n",
    "    \n",
    "    print(result_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77b2d7-dede-49aa-b2bd-501fc72f65df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...... Modelling\n",
      "n_class : 40\n",
      "select LSTM_attention\n",
      "dataset.get_eeg_shape()  torch.Size([11965, 440, 128])\n",
      "Model train device: cuda:1\n",
      "The model LSTM_attention has 44,072 trainable parameters\n",
      "...Do trainning for : 500 epochs\n",
      "Epoch: 01/500 |\tTrain Loss: 3.68794   | Train Acc: 3.37%   |\t Val. Loss: 3.66987  | Val. Acc: 4.34%   |\t LR: 0.001 |\tBest epoch : None\n",
      "Epoch: 02/500 |\tTrain Loss: 3.63613   | Train Acc: 5.67%   |\t Val. Loss: 3.62475  | Val. Acc: 5.10%   |\t LR: 0.001 |\tBest epoch : 0\n",
      "Epoch: 03/500 |\tTrain Loss: 3.57322   | Train Acc: 7.41%   |\t Val. Loss: 3.55721  | Val. Acc: 6.35%   |\t LR: 0.001 |\tBest epoch : 1\n",
      "Epoch: 04/500 |\tTrain Loss: 3.47579   | Train Acc: 8.58%   |\t Val. Loss: 3.46529  | Val. Acc: 7.85%   |\t LR: 0.001 |\tBest epoch : 2\n",
      "Epoch: 05/500 |\tTrain Loss: 3.35846   | Train Acc: 9.83%   |\t Val. Loss: 3.33128  | Val. Acc: 8.86%   |\t LR: 0.001 |\tBest epoch : 3\n",
      "Epoch: 06/500 |\tTrain Loss: 3.23967   | Train Acc: 11.25%   |\t Val. Loss: 3.22088  | Val. Acc: 10.03%   |\t LR: 0.001 |\tBest epoch : 4\n",
      "Epoch: 07/500 |\tTrain Loss: 3.12256   | Train Acc: 11.79%   |\t Val. Loss: 3.13793  | Val. Acc: 10.94%   |\t LR: 0.001 |\tBest epoch : 5\n",
      "Epoch: 08/500 |\tTrain Loss: 3.03883   | Train Acc: 12.96%   |\t Val. Loss: 3.03219  | Val. Acc: 11.70%   |\t LR: 0.001 |\tBest epoch : 6\n",
      "Epoch: 09/500 |\tTrain Loss: 2.95131   | Train Acc: 13.82%   |\t Val. Loss: 2.96934  | Val. Acc: 12.11%   |\t LR: 0.001 |\tBest epoch : 7\n",
      "Epoch: 10/500 |\tTrain Loss: 2.90929   | Train Acc: 14.52%   |\t Val. Loss: 2.98281  | Val. Acc: 11.70%   |\t LR: 0.001 |\tBest epoch : 8\n",
      "Epoch: 11/500 |\tTrain Loss: 2.87820   | Train Acc: 14.18%   |\t Val. Loss: 2.88478  | Val. Acc: 12.11%   |\t LR: 0.0005 |\tBest epoch : 8\n",
      "Epoch: 12/500 |\tTrain Loss: 2.78996   | Train Acc: 15.83%   |\t Val. Loss: 2.84316  | Val. Acc: 12.45%   |\t LR: 0.0005 |\tBest epoch : 10\n",
      "Epoch: 13/500 |\tTrain Loss: 2.82595   | Train Acc: 15.01%   |\t Val. Loss: 2.98523  | Val. Acc: 11.61%   |\t LR: 0.0005 |\tBest epoch : 11\n",
      "Epoch: 14/500 |\tTrain Loss: 2.86327   | Train Acc: 14.98%   |\t Val. Loss: 2.91133  | Val. Acc: 12.11%   |\t LR: 0.0005 |\tBest epoch : 11\n",
      "Epoch: 15/500 |\tTrain Loss: 2.82110   | Train Acc: 14.67%   |\t Val. Loss: 2.87708  | Val. Acc: 12.70%   |\t LR: 0.0005 |\tBest epoch : 11\n",
      "Epoch: 16/500 |\tTrain Loss: 2.76523   | Train Acc: 15.64%   |\t Val. Loss: 2.82630  | Val. Acc: 12.61%   |\t LR: 0.0005 |\tBest epoch : 11\n",
      "Epoch: 17/500 |\tTrain Loss: 2.76144   | Train Acc: 15.60%   |\t Val. Loss: 2.81517  | Val. Acc: 12.70%   |\t LR: 0.0005 |\tBest epoch : 15\n",
      "Epoch: 18/500 |\tTrain Loss: 2.74635   | Train Acc: 16.11%   |\t Val. Loss: 2.78237  | Val. Acc: 12.95%   |\t LR: 0.0005 |\tBest epoch : 16\n",
      "Epoch: 19/500 |\tTrain Loss: 2.70960   | Train Acc: 16.77%   |\t Val. Loss: 2.78315  | Val. Acc: 11.78%   |\t LR: 0.0005 |\tBest epoch : 17\n",
      "Epoch: 20/500 |\tTrain Loss: 2.71158   | Train Acc: 16.56%   |\t Val. Loss: 2.78356  | Val. Acc: 13.28%   |\t LR: 0.0005 |\tBest epoch : 17\n",
      "Epoch: 21/500 |\tTrain Loss: 2.68005   | Train Acc: 17.01%   |\t Val. Loss: 2.74976  | Val. Acc: 13.95%   |\t LR: 0.00025 |\tBest epoch : 17\n",
      "Epoch: 22/500 |\tTrain Loss: 2.65889   | Train Acc: 17.17%   |\t Val. Loss: 2.75217  | Val. Acc: 13.28%   |\t LR: 0.00025 |\tBest epoch : 20\n",
      "Epoch: 23/500 |\tTrain Loss: 2.65402   | Train Acc: 17.61%   |\t Val. Loss: 2.74806  | Val. Acc: 13.78%   |\t LR: 0.00025 |\tBest epoch : 20\n",
      "Epoch: 24/500 |\tTrain Loss: 2.63943   | Train Acc: 17.69%   |\t Val. Loss: 2.71239  | Val. Acc: 13.95%   |\t LR: 0.00025 |\tBest epoch : 22\n"
     ]
    }
   ],
   "source": [
    "dataset = Spam_EEG_ds(device,  participant_id=participant_id)\n",
    "train_loader, val_loader  = train_test_split_ds(dataset, batch_size=batch_size)\n",
    "\n",
    "# %%\n",
    "org_eeg_shape = dataset.get_eeg_shape()\n",
    "get_acc_save_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1e945-12c8-4c09-a73b-fd083d92869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model running time : { (time.time()-start_time)/(60)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabca618-2d53-4feb-a2af-90aa8da2d1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"participant_id :\", participant_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jakrapop_env39",
   "language": "python",
   "name": "jakrapop_env39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
